{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4045,"status":"ok","timestamp":1714639642335,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-180},"id":"yWfLcvf0l49c","metadata":{},"outputId":"d33a0683-a430-44da-80f7-434de7ec09bc"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","import pandas as pd\n","device = torch.device(\"cuda\")\n","\n","from train_autoencoders import*\n","from coupled_learning_mlp import*\n","from GAN import*\n","from Mlp import*\n","from Testing_function import*\n","from Autoencoders import*\n","from train_mapping_functions import*\n","from coupled_learning_new import*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8147,"status":"ok","timestamp":1714641052091,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-180},"id":"Ns8byVIZcZ6i","metadata":{},"outputId":"e4143714-bfd2-40e8-edc5-795e7d587020"},"outputs":[],"source":["df=pd.read_csv('/home/alexgi/Workspace/REC_SYSTEMS/My_codes/Data/Rs_no3.csv')\n","Movies_source=df.values       # movies x users\n","del df\n","df=pd.read_csv('/home/alexgi/Workspace/REC_SYSTEMS/My_codes/Data/Rt_no3.csv')\n","Movies_target=df.values   \n","\n","Movies_source=Movies_source/5\n","Movies_target=Movies_target/5\n","print('source domain sparsity:', np.count_nonzero(Movies_source==0)/(Movies_source.shape[0]*Movies_source.shape[1])*100)\n","print('target domain sparsity:', np.count_nonzero(Movies_target==0)/(Movies_target.shape[0]*Movies_target.shape[1])*100)\n","\n","Movies_source = Movies_source[:,1:Movies_source.shape[1]]\n","Movies_target = Movies_target[:,1:Movies_target.shape[1]]"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class MovieDataset(Dataset):\n","    def __init__(self, movies):\n","        self.movies = movies\n","\n","    def __len__(self):\n","        return len(self.movies)\n","\n","    def __getitem__(self, index):\n","        return index, self.movies[index, :]\n","\n","class UserDataset(Dataset):\n","    def __init__(self, users):\n","        self.users = users\n","\n","    def __len__(self):\n","        return self.users.shape[0]\n","\n","    def __getitem__(self, index):\n","        return index, self.users[index, :]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Load Training and Testing data\n","\n","Movies_source_train, Movies_source_test, Movies_target_train, Movies_target_test = train_test_split(Movies_source, Movies_target, test_size=0.2)\n","Users_source_train = Movies_source_train.T\n","Users_source_test  = Movies_source_test.T\n","\n","Users_target_train = Movies_target_train.T\n","Users_target_test  = Movies_target_test.T\n","\n","Users_source=Movies_source.T\n","# del df,Movies_s,Movies_t\n"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["\n","num_epochs = 120\n","learning_rate = 1e-3\n","l2_w = 1e-8\n","\n","batch_size_m = 64\n","batch_size_u = 64\n","batch_size = batch_size_m\n","\n","layer_1_dim = 64\n","layer_2_dim = 32\n","layer_3_dim = 16\n","\n","##Source Domain ----------------------------------------------------------------------------------------------------------------------------------\n","\n","dataloader_movies = DataLoader(MovieDataset(Movies_source_train), batch_size=batch_size_m, shuffle=False)\n","dataloader_users = DataLoader(UserDataset(Users_source_train), batch_size=batch_size_m, shuffle=False)\n","\n","\n","model_encoder_m_s = encoder(Users_source_train.shape[0],layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","model_decoder_m_s = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Users_source_train.shape[0]).cuda()\n","\n","model_encoder_u_s = encoder(Movies_source_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","model_decoder_u_s = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_source_train.shape[0]).cuda()\n","\n","train_latentfactors_autoencoders_new(model_encoder_m_s, model_decoder_m_s, model_encoder_u_s, model_decoder_u_s, num_epochs,\n","                                learning_rate, dataloader_movies, dataloader_users, torch.tensor(Movies_source_train), device, l2_w, \n","                                weight_loss_movies = 0.5, weight_loss_users = 0.5, weight_loss_latent = 0.5)\n","\n","del dataloader_movies, dataloader_users\n","\n","##Traget DOmain ---------------------------------------------------------------------------------------------------------------------------------\n","\n","dataloader_movies = DataLoader(MovieDataset(Movies_target_train), batch_size=batch_size_m, shuffle=False)\n","dataloader_users = DataLoader(UserDataset(Users_target_train), batch_size=batch_size_m, shuffle=False)\n","\n","model_encoder_m_t = encoder(Users_target_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","model_decoder_m_t = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Users_target_train.shape[0]).cuda()\n","\n","model_encoder_u_t = encoder(Movies_target_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","model_decoder_u_t = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_target_train.shape[0]).cuda()\n","\n","train_latentfactors_autoencoders_new(model_encoder_m_t, model_decoder_m_t, model_encoder_u_t, model_decoder_u_t, num_epochs,\n","                                learning_rate, dataloader_movies, dataloader_users, torch.tensor(Movies_target_train), device, l2_w, \n","                                weight_loss_movies = 0.5, weight_loss_users = 0.5, weight_loss_latent = 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["#Dataloader for the mapping MLP ----------------------------------------------------------------------------------------------------\n","dataset = TensorDataset(torch.tensor(Movies_source_train), torch.tensor(Movies_target_train))\n","dataloader_Coupled = DataLoader(dataset, 64, shuffle=True)\n","num_epochs = 50\n","learning_rate = 1e-3\n","l2_w = 1e-8\n","\n","generator = Generator(input_size=layer_3_dim, hidden_size=layer_3_dim*2, output_size=layer_3_dim).to(device)\n","discriminator = Discriminator(input_size=layer_3_dim, hidden_size=layer_3_dim*2).to(device)\n","train_GAN(generator, discriminator, model_encoder_m_s, model_encoder_m_t, dataloader_Coupled, device, num_epochs, learning_rate)\n","\n","\n","# generator = mlp_network(layer_3_dim, 32).cuda()\n","# train_MLP(generator,  model_encoder_m_s, model_encoder_m_t, num_epochs, learning_rate, dataloader_Coupled, device, l2_w)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataloader_movies_source = DataLoader(MovieDataset(Movies_source_train), batch_size=64, shuffle=True)\n","dataloader_users_target = DataLoader(UserDataset(Users_target_train), batch_size=64, shuffle=True)\n","\n","\n","num_epochs = 100\n","learning_rate = 1e-4\n","l2_w = 1e-9\n","loss_rmse, loss_mae, prec, recall = train_Coupled_model_Matrix_factorization_new_version(generator, model_encoder_m_s, model_encoder_u_t, num_epochs, learning_rate,\n","                                            dataloader_movies_source, dataloader_users_target, device, Movies_source_test, Movies_target_test,\n","                                            l2_w, Users_target_train, Movies_target_train)\n","\n","print((loss_rmse), (loss_mae), prec, recall)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# Path to your CSV file\n","csv_path = \"/home/alexgi/Workspace/REC_SYSTEMS/My_codes/Data/Rs_no1.csv\"\n","\n","rating_matrix = pd.read_csv(csv_path, index_col=0)\n","\n","# # Prepare to collect non-zero entries\n","# data_list = []\n","\n","# # Iterate over each item in the DataFrame\n","# for item_id, row in rating_matrix.iterrows():\n","#     for user_id, rating in row.iteritems():\n","#         if rating != 0:  # Assuming zero is the value that signifies 'no rating'\n","#             data_list.append({'userId': user_id, 'itemId': item_id, 'rating': rating})\n","\n","# # Convert list to DataFrame\n","# filtered_data = pd.DataFrame(data_list)\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
