{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4045,"status":"ok","timestamp":1714639642335,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-180},"id":"yWfLcvf0l49c","metadata":{},"outputId":"d33a0683-a430-44da-80f7-434de7ec09bc"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","import pandas as pd\n","device = torch.device(\"cuda\")\n","\n","from train_autoencoders import*\n","from coupled_learning_mlp import*\n","from GAN import*\n","from Mlp import*\n","from Testing_function import*\n","from Autoencoders import*\n","from train_mapping_functions import*\n","from coupled_learning_new import*\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","class MovieDataset(Dataset):\n","    def __init__(self, movies):\n","        self.movies = movies\n","\n","    def __len__(self):\n","        return len(self.movies)\n","\n","    def __getitem__(self, index):\n","        return index, self.movies[index, :]\n","\n","class UserDataset(Dataset):\n","    def __init__(self, users):\n","        self.users = users\n","\n","    def __len__(self):\n","        return self.users.shape[0]\n","\n","    def __getitem__(self, index):\n","        return index, self.users[index, :]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8147,"status":"ok","timestamp":1714641052091,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-180},"id":"Ns8byVIZcZ6i","metadata":{},"outputId":"e4143714-bfd2-40e8-edc5-795e7d587020"},"outputs":[{"name":"stdout","output_type":"stream","text":["source domain sparsity: 99.35004475168513\n","target domain sparsity: 98.77889420673183\n"]}],"source":["df=pd.read_csv('/home/alexgi/Workspace/REC_SYSTEMS/My_codes/Data/Rs_no3.csv')\n","Movies_source=df.values       # movies x users\n","del df\n","df=pd.read_csv('/home/alexgi/Workspace/REC_SYSTEMS/My_codes/Data/Rt_no3.csv')\n","Movies_target=df.values   \n","\n","Movies_source=Movies_source/5\n","Movies_target=Movies_target/5\n","print('source domain sparsity:', np.count_nonzero(Movies_source==0)/(Movies_source.shape[0]*Movies_source.shape[1])*100)\n","print('target domain sparsity:', np.count_nonzero(Movies_target==0)/(Movies_target.shape[0]*Movies_target.shape[1])*100)\n","\n","Movies_source = Movies_source[:,1:Movies_source.shape[1]]\n","Movies_target = Movies_target[:,1:Movies_target.shape[1]]"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1979522556066513 0.155105859041214 0.8669704501518916 0.97970173985087\n"]}],"source":["#SIMPLE METHOD\n","\n","batch_size = 32\n","\n","layer_1_dim = 64\n","layer_2_dim = 32\n","layer_3_dim = 16\n","\n","\n","frmse = []\n","fmae = []\n","\n","for i in range(1):\n","\n","   Movies_source_train, Movies_source_test, Movies_target_train, Movies_target_test = train_test_split(Movies_source, Movies_target, test_size=0.2)\n","   dataset = (torch.from_numpy((np.concatenate((Movies_source_train, Movies_source_test)))))\n","   dataloader_train_source = DataLoader(dataset, batch_size, shuffle=False)\n","\n","   dataset = (torch.from_numpy(Movies_target_train))\n","   dataloader_train_target = DataLoader(dataset, batch_size, shuffle=False)\n","\n","   dataset = TensorDataset(torch.from_numpy(Movies_source_train), torch.from_numpy(Movies_target_train))\n","   dataloader_Coupled = DataLoader(dataset, batch_size, shuffle=False)\n","\n","   # ============ Train Autoencoders ==============\n","   model_encoder_s = encoder(Movies_source_train.shape[1], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","   model_decoder_s = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_source_train.shape[1]).cuda()\n","\n","   model_encoder_t = encoder(Movies_target_train.shape[1], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","   model_decoder_t = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_target_train.shape[1]).cuda()\n","\n","   noise_level = 0\n","   learning_rate = 1e-3\n","   l2_w = 1e-8\n","   num_epochs = 30\n","\n","   train_autoencoders(model_encoder_s, model_decoder_s, num_epochs,\n","                                                    learning_rate, dataloader_train_source, device, l2_w, noise_level)\n","   train_autoencoders(model_encoder_t, model_decoder_t, num_epochs,\n","                                                    learning_rate, dataloader_train_target, device, l2_w, noise_level)\n","\n","\n","   # #GAN_based \n","\n","   generator = Generator(input_size=16, hidden_size=32, output_size=16).to(device)\n","   discriminator = Discriminator(input_size=16, hidden_size=32).to(device)\n","   train_GAN(generator, discriminator, model_encoder_s, model_encoder_t, dataloader_Coupled, device, epochs=15, learning_rate=1e-03)\n","   # r3,m3 = test_data(model_encoder_s, generator, model_decoder_t, Movies_source_test, Movies_target_test, 0)\n","   # print('Before Coupled Learning')\n","   # print(r3, m3)\n","   \n","   \n","   # ============ Multilayer Percepton Training ==============\n","   # num_epochs = 10\n","   # learning_rate = 1e-3\n","   # l2_w = 1e-5\n","   # generator = mlp_network(layer_3_dim, 32).cuda()\n","   # train_MLP(generator, model_encoder_s, model_encoder_t, num_epochs, learning_rate, dataloader_Coupled, device, l2_w)\n","   # r3,m3=test_data(model_encoder_s, generator, model_decoder_t, Movies_source_test, Movies_target_test)\n","   # #print(r3, m3)\n","   \n","   \n","   \n","   # ============ Coupled =====================================\n","   # print('After Coupled Learning')\n","   num_epochs = 50\n","   learning_rate = 1e-4\n","   l2_w = 1e-8\n","   rmse_loss, mae_loss, precision, recall = train_Coupled_model(generator, model_encoder_s, model_decoder_t,\n","                                                                num_epochs, learning_rate, dataloader_Coupled,\n","                                                               device, Movies_source_test, Movies_target_test, l2_w, noise_level)\n","   # print(rmse_loss, mae_loss )\n","   \n","   \n","   frmse.append(rmse_loss)\n","   fmae.append(mae_loss)\n","\n","\n","print(np.mean(frmse), np.mean(fmae), precision, recall)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["#MATRIX_FACTORIZATION\n","\n","mf_rmse = []\n","mf_mae = []\n","\n","for i in range(1):\n","\n","    #Load Training and Testing data\n","\n","    Movies_source_train, Movies_source_test, Movies_target_train, Movies_target_test = train_test_split(Movies_source, Movies_target, test_size=0.2)\n","    Users_source_train = Movies_source_train.T\n","    Users_source_test  = Movies_source_test.T\n","\n","    Users_target_train = Movies_target_train.T\n","    Users_target_test  = Movies_target_test.T\n","\n","    Users_source=Movies_source.T\n","    # del df,Movies_s,Movies_t\n","\n","    num_epochs = 10\n","    learning_rate = 1e-3\n","    l2_w = 1e-5\n","\n","    batch_size_m = 500\n","    batch_size_u = 500\n","    batch_size = batch_size_m\n","\n","    layer_1_dim = 64\n","    layer_2_dim = 32\n","    layer_3_dim = 16\n","\n","\n"," \n","\n","    # Dataloader source domain\n","    dataset = (torch.tensor(Movies_source_train))\n","    dataloader_movies = DataLoader(dataset, batch_size_m, shuffle=False)\n","    dataset = (torch.tensor(Users_source_train))\n","    dataloader_users = DataLoader(dataset, batch_size_u, shuffle=False)\n","\n","    model_encoder_m_s = encoder(Users_source_train.shape[0],layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","    model_decoder_m_s = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Users_source_train.shape[0]).cuda()\n","\n","    model_encoder_u_s = encoder(Movies_source_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","    model_decoder_u_s = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_source_train.shape[0]).cuda()\n","\n","    train_latentfactors_autoencoders(model_encoder_m_s, model_decoder_m_s, model_encoder_u_s, model_decoder_u_s, num_epochs,\n","                                    learning_rate, dataloader_movies, dataloader_users, device, l2_w,  batch_size_u)\n","\n","    del dataset,dataloader_movies,dataloader_users\n","\n","    #Dataloader Target domain --------------------------------------------------------------------------------------------------------\n","\n","    dataset = (torch.tensor(Movies_target_train))\n","    dataloader_movies = DataLoader(dataset, batch_size_m, shuffle=False)\n","    dataset = (torch.tensor(Users_target_train))\n","    dataloader_users = DataLoader(dataset, batch_size_u, shuffle=False)\n","\n","    model_encoder_m_t = encoder(Users_target_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","    model_decoder_m_t = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Users_target_train.shape[0]).cuda()\n","\n","    model_encoder_u_t = encoder(Movies_target_train.shape[0], layer_1_dim, layer_2_dim, layer_3_dim).cuda()\n","    model_decoder_u_t = decoder(layer_3_dim, layer_2_dim, layer_1_dim, Movies_target_train.shape[0]).cuda()\n","\n","    train_latentfactors_autoencoders(model_encoder_m_t, model_decoder_m_t, model_encoder_u_t,\n","                                    model_decoder_u_t, num_epochs, learning_rate, dataloader_movies,\n","                                    dataloader_users, device, l2_w,  batch_size_u)\n","\n","    del dataset, dataloader_movies, dataloader_users\n","\n","    #Dataloader for the mapping MLP ----------------------------------------------------------------------------------------------------\n","    dataset = TensorDataset(torch.tensor(Movies_source_train), torch.tensor(Movies_target_train))\n","    dataloader_Coupled = DataLoader(dataset, batch_size, shuffle=False)\n","    num_epochs = 50\n","    learning_rate = 1e-3\n","    l2_w = 1e-5\n","    \n","    generator = Generator(input_size=16, hidden_size=32, output_size=16).to(device)\n","    discriminator = Discriminator(input_size=16, hidden_size=32).to(device)\n","    train_GAN(generator, discriminator, model_encoder_m_s, model_encoder_m_t, dataloader_Coupled, device, epochs=10, learning_rate= 1e-3)\n","\n","    # generator = mlp_network(layer_3_dim, 16).cuda()\n","    # train_MLP(generator, model_encoder_m_s, model_encoder_m_t, num_epochs,\n","    #                       learning_rate, dataloader_Coupled, device, l2_w)\n","\n","    # r3,m3 = test_data_Matrix_factorization(model_encoder_m_s, generator, model_encoder_u_t,\n","    #                   Movies_source_test, Movies_target_test, Users_target_train)\n","\n","    del dataset, dataloader_Coupled\n","    #print(\"before Coupling\")\n","    # print(r3, m3)\n","\n","    # Coupled Model --------------------------------------------------------------------------------------------------------------------\n","    dataset = TensorDataset(torch.tensor(Movies_source_train), torch.tensor(Movies_target_train))\n","    dataloader_movies = DataLoader(dataset, batch_size_m, shuffle=False)\n","    dataset = (torch.tensor(Users_target_train))\n","    dataloader_users = DataLoader(dataset, batch_size_u, shuffle=False)\n","\n","    num_epochs = 100\n","    learning_rate = 1e-3\n","    l2_w = 1e-8\n","    loss_rmse, loss_mae, prec, recall = train_Coupled_model_Matrix_factorization(generator, model_encoder_m_s, model_encoder_u_t, num_epochs, learning_rate,\n","                                              dataloader_movies, dataloader_users, device, Movies_source_test, Movies_target_test,\n","                                              l2_w, Users_target_train, Movies_target_train, batch_size_m, batch_size_u)\n","    mf_rmse.append(loss_rmse)\n","    mf_mae.append(loss_mae)\n","\n","print(np.mean(mf_rmse), np.mean(mf_mae), prec, recall)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BogTlZk8ga2p"},"source":["Count the sparsity levels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":10077,"status":"error","timestamp":1714635884350,"user":{"displayName":"Alexandros Gkillas","userId":"06090391217186072891"},"user_tz":-180},"id":"cT6Rv81Lnr6h","outputId":"2118d640-e84b-49cc-d83c-42c93aeb6f5a"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/DATA_Recommendation/Rs_no4.csv')\n","Movies_source=df.values       # movies x users\n","df=pd.read_csv('/content/drive/MyDrive/DATA_Recommendation/Rt_no4.csv')\n","Movies_target=df.values\n","\n","Movies_source_train, Movies_source_test, Movies_target_train, Movies_target_test = train_test_split(Movies_source, Movies_target, test_size=0.2)\n","\n","\n","\n","n = 5000\n","index = np.random.choice(Movies_source.shape[1], n, replace=False)\n","Rsource = Movies_source\n","#Rsource[:,index] = 0\n","np.count_nonzero(Rsource)/(Rsource.shape[0]*Rsource.shape[1])*100\n","\n","\n","index = np.random.choice(Movies_target.shape[1], n, replace=False)\n","Rtarget = Movies_target\n","#Rtarget[:,index] = 0\n","np.count_nonzero(Rtarget)/(Rtarget.shape[0]*Rtarget.shape[1])*100\n","\n","np.amax(Rsource)\n","\n","\n","pd.DataFrame(Rsource).to_csv(\"/content/drive/MyDrive/DATA/Rs_20.csv\")\n","\n","pd.DataFrame(Rtarget).to_csv(\"/content/drive/MyDrive/DATA/Rt_20.csv\")"]},{"cell_type":"markdown","metadata":{"id":"oCgseWxAlWVt"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
